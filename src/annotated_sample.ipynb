{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import duckdb\n",
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import classification_report, f1_score, balanced_accuracy_score\n",
    "import tiktoken\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(\":memory:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3920520,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"SELECT COUNT(*) FROM '../data/wildchat.parquet'\").fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(826406,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"SELECT COUNT(DISTINCT conversation_hash) FROM '../data/wildchat.parquet'\").fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtered sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265419,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"SELECT COUNT(*) FROM '../data/wildchat.parquet' WHERE country = 'United States' AND role = 'user' AND language = 'English'\").fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(DISTINCT conversation_hash)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>146164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(DISTINCT conversation_hash)\n",
       "0                             146164"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"SELECT COUNT(DISTINCT conversation_hash) FROM '../data/wildchat.parquet' WHERE country = 'United States' AND role = 'user' AND language = 'English'\").fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random annotation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>count_star()</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   classification  count_star()\n",
       "0               0           992\n",
       "1               1             8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"SELECT classification, COUNT(*) FROM '../data/sample_for_annotation_annotated.csv' GROUP BY classification\").fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targeted search sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/searched_news.txt\", \"r\") as f:\n",
    "    records = f.read()\n",
    "\n",
    "print(len(records.split(\"---\")) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM performance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./prompts/classification.txt\", \"r\") as f:\n",
    "    prompt = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare human annotations for LLM performance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = con.execute(\"SELECT content, classification FROM '../data/sample_for_annotation_annotated.csv'\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "searched = pd.DataFrame([r.strip() for r in records.split(\"---\")[:-1]], columns=[\"content\"])\n",
    "searched[\"classification\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.concat([annotations, searched])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classification\n",
       "0    992\n",
       "1     66\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.classification.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LLM evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1058/1058 [28:41<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "outputs_mini = []\n",
    "outputs_o = []\n",
    "for _, s in tqdm(annotations.iterrows(), total=len(annotations)):\n",
    "    resp = llm.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"[MESSAGE]{s.content}[\\\\MESSAGE]\"},\n",
    "        ]\n",
    "    )\n",
    "    outputs_mini.append(resp.choices[0].message.content)\n",
    "    resp = llm.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"[MESSAGE]{s.content}[\\\\MESSAGE]\"},\n",
    "        ]\n",
    "    )\n",
    "    outputs_o.append(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[\"gpt-4o-mini\"] = outputs_mini\n",
    "annotations[\"gpt-4o\"] = outputs_o\n",
    "annotations.to_csv(\"../data/sample_for_annotation_annotated_llm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_classification(output):\n",
    "    clf = output.split(\"\\n\")[0]\n",
    "    try:\n",
    "        return int(clf)\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[\"clf_gpt-4o-mini\"] = annotations[\"gpt-4o-mini\"].apply(extract_classification)\n",
    "annotations[\"clf_gpt-4o\"] = annotations[\"gpt-4o\"].apply(extract_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to 0 if not found (1 record)\n",
    "annotations.loc[annotations[\"clf_gpt-4o\"].isna(), \"clf_gpt-4o\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clf_gpt-4o-mini\n",
       "0    974\n",
       "1     84\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[\"clf_gpt-4o-mini\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clf_gpt-4o\n",
       "0.0    940\n",
       "1.0    118\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[\"clf_gpt-4o\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       992\n",
      "           1       0.71      0.91      0.80        66\n",
      "\n",
      "    accuracy                           0.97      1058\n",
      "   macro avg       0.85      0.94      0.89      1058\n",
      "weighted avg       0.98      0.97      0.97      1058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(annotations[\"classification\"], annotations[\"clf_gpt-4o-mini\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       992\n",
      "           1       0.54      0.97      0.70        66\n",
      "\n",
      "    accuracy                           0.95      1058\n",
      "   macro avg       0.77      0.96      0.83      1058\n",
      "weighted avg       0.97      0.95      0.95      1058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(annotations[\"classification\"], annotations[\"clf_gpt-4o\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9732161297828683\n",
      "0.9538368811813375\n",
      "0.9424486803519061\n",
      "0.957630742913001\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(annotations[\"classification\"], annotations[\"clf_gpt-4o-mini\"], average=\"weighted\"))\n",
    "print(f1_score(annotations[\"classification\"], annotations[\"clf_gpt-4o\"], average=\"weighted\"))\n",
    "print(balanced_accuracy_score(annotations[\"classification\"], annotations[\"clf_gpt-4o-mini\"]))\n",
    "print(balanced_accuracy_score(annotations[\"classification\"], annotations[\"clf_gpt-4o\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost estimate for entire sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 66777553\n",
      "Cost: $10.02\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "all_user_messages = con.execute(\"SELECT content FROM '../data/wildchat.parquet' WHERE country = 'United States' AND role = 'user' AND language = 'English'\").fetchdf()\n",
    "all_tokens = sum([len(enc.encode(m, disallowed_special=())) for m in all_user_messages.content])\n",
    "print(f\"Total tokens: {all_tokens}\")\n",
    "print(f\"Cost: ${all_tokens * 0.15 / 1_000_000:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 32158\n",
      "Cost per record: $0.00\n",
      "Total cost: $5.02\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "all_outputs = con.execute(\"\"\"SELECT \"gpt-4o-mini\" FROM '../data/sample_for_annotation_annotated_llm.csv'\"\"\").fetchdf()\n",
    "all_tokens = sum([len(enc.encode(m, disallowed_special=())) for m in all_outputs[\"gpt-4o-mini\"]])\n",
    "print(f\"Total tokens: {all_tokens}\")\n",
    "cost_per_record = all_tokens * 0.6 / 1_000_000 / len(all_outputs)\n",
    "print(f\"Cost per record: ${cost_per_record:.2f}\")\n",
    "print(f\"Total cost: ${cost_per_record * len(all_user_messages):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM annotate entire sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sample = con.execute(\"\"\"SELECT conversation_hash, content FROM '../data/wildchat.parquet' \n",
    "            WHERE country = 'United States' AND role = 'user' AND language = 'English'\n",
    "            \"\"\").fetch_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 7\n",
      "Rows in first chunk: 40000\n",
      "Rows in last chunk: 25419\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of chunks needed\n",
    "num_chunks = len(full_sample) // 40000 + (1 if len(full_sample) % 40000 != 0 else 0)\n",
    "\n",
    "# Create a list to store the chunks\n",
    "chunks = []\n",
    "\n",
    "# Split the dataframe into chunks\n",
    "for i in range(num_chunks):\n",
    "    start_idx = i * 40000\n",
    "    end_idx = min((i + 1) * 40000, len(full_sample))\n",
    "    chunk = full_sample.iloc[start_idx:end_idx].copy()\n",
    "    chunks.append(chunk)\n",
    "\n",
    "print(f\"Number of chunks created: {len(chunks)}\")\n",
    "print(f\"Rows in first chunk: {len(chunks[0])}\")\n",
    "print(f\"Rows in last chunk: {len(chunks[-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_batch_submission(row):\n",
    "    obj = {\n",
    "        \"custom_id\": str(row.name),\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"[MESSAGE]{row.content}[\\\\MESSAGE]\"},\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply formatting to each chunk\n",
    "chunks = [chunk.apply(format_for_batch_submission, axis=1) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save each chunk as a .jsonl file\n",
    "for i, chunk in enumerate(chunks):\n",
    "    with open(f\"../data/batches/batch_{i}.jsonl\", \"w\") as f:\n",
    "        for _, s in chunk.items():\n",
    "            f.write(json.dumps(s) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batch files: 7\n"
     ]
    }
   ],
   "source": [
    "# list all .jsonl files in the batches folder\n",
    "batch_files = list(pathlib.Path(\"../data/batches\").glob(\"*.jsonl\"))\n",
    "print(f\"Number of batch files: {len(batch_files)}\")\n",
    "\n",
    "batch_input_refs = []\n",
    "\n",
    "for batch_file in batch_files:\n",
    "    batch_input_file = llm.files.create(\n",
    "        file=open(batch_file, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "    batch_input_refs.append(batch_input_file.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file-noBpGJ1Y3yNuE7VaICcICs64',\n",
       " 'file-cBJRySmzxfntYyg2DH0b41hb',\n",
       " 'file-PzohLzh5th966r5WWmpGjYth',\n",
       " 'file-YlEkC5TAg8LNOD7OaVs9zSES',\n",
       " 'file-ECcElQVgppt7WNgq5tGwPQPI',\n",
       " 'file-CXt4tZjV1s05gZVlGkZWy9vv',\n",
       " 'file-fC9w6JcSA7ULiVq3L0H2WFrF']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in batch_input_refs[1:]:\n",
    "    llm.batches.create(\n",
    "        input_file_id=b,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"description\": \"Batch annotation of user messages in WildChat\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
